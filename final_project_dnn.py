# -*- coding: utf-8 -*-
"""HTML_Final_Project_DNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f7RbZ-4qe27gHeW_f1057XjR8gGpaJEY

# **Download Data**
"""

tr_path = 'trainval_data.csv'  # path to training data
tt_path = 'Test_data.csv'

"""# **Import Some Packages**"""

# PyTorch
from base64 import encode
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# For data preprocess
import numpy as np
import csv
import os

# sklearn
from sklearn.metrics import confusion_matrix,  f1_score
from sklearn.preprocessing import Normalizer

# other
import math


myseed = 1126  # set a random seed for reproducibility
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(myseed)
torch.manual_seed(myseed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(myseed)
"""# **Utilities**"""

def get_device():
    ''' Get device (if GPU is available, use GPU) '''
    return 'cuda' if torch.cuda.is_available() else 'cpu'
# encode y, which is our objective 'Churn Category'
def encode_y(data):
    from sklearn.preprocessing import OrdinalEncoder
    enc = OrdinalEncoder(categories=[['No Churn', 'Competitor', 'Dissatisfaction', 'Attitude', 'Price', 'Other']])
    y = enc.fit_transform(data[:, 44:45])
    for i in range((len(y))):
        y[i] = int(y[i])
    return y
#encode other non-numerical features
from category_encoders.target_encoder import TargetEncoder
def encode_other(data, y, feats):
    for i in feats:
        enc = TargetEncoder()
        data[:, i:i+1] = enc.fit_transform(data[:, i:i+1], y)
    return data[:, feats]

def encode_test(data_train, y, data, feats):
    
    for i in feats:
        enc = TargetEncoder()
        enc.fit(data_train[:, i:i+1], y)
        data[:, i:i+1] = enc.transform(data[:, i:i+1])
    return data[:, feats]
    # from sklearn.preprocessing import OrdinalEncoder
    # enc = OrdinalEncoder()
    # for i in feats:
    #     data[:, i:i+1] = enc.fit_transform(data[:, i:i+1])
    # return data[:, feats]

# output prediction 
def make_pred(pred, test_id):
    with open('pred.csv', 'w', newline='') as fp:
        writer = csv.writer(fp)
        writer.writerow(['Customer ID', 'Churn Category'])
        for i, p in enumerate(pred): 
            writer.writerow([test_id[i, 0], int(p)])

"""# **Dataset**

The `CustomerDataset` below does:
* read `.csv` files
* extract features
* split dataset into train/val sets

"""

class CustomerDataset(Dataset):
    ''' Dataset for loading and preprocessing the dataset '''
    def __init__(self, path, mode='train', up = False, path2 = None):
        self.mode = mode
        self.up = up
        # Read data into numpy arrays
        with open(path, 'r') as fp:
            data = list(csv.reader(fp))
            data = np.array(data[1:])[:, 1:]            

        feats = [19, 22] + list(range(38,44))
        feats_la = [15, 17, 20, 21, 24, 25, 34, 35] + list(range(27,31))

        if mode == 'test':
            with open(path2, 'r') as fp:
                data_train = list(csv.reader(fp))
                self.test_id = np.array(data_train[1:])
                data_train = np.array(data_train[1:])[:, 1:]
                y = encode_y(data_train)
            # Testing data
            data_la  = encode_test(data_train, y, data, feats_la).astype(float)
            data = data[:, feats].astype(float)
            data = np.hstack((data, data_la))
            data = np.nan_to_num(data)
            data = Normalizer().fit_transform(data)

            self.data = torch.FloatTensor(data)
        else:
            # Training data (train/val sets)
            target = encode_y(data)
            data_la = encode_other(data, target, feats_la).astype(float)
            data = data[:, feats].astype(float)
            data = np.hstack((data, data_la))
            data = np.nan_to_num(data)
            data = Normalizer().fit_transform(data)
            
            # Splitting training data into train & val sets (9:1)
            if mode == 'train':
                indices = [i for i in range(len(data)) if i % 10 != 0]
                if up == True:
                    
                    # upsampling for training data
                    data = torch.FloatTensor(data)
                    target = torch.LongTensor(target)

                    from imblearn.over_sampling import SMOTE, BorderlineSMOTE
                    smt = BorderlineSMOTE(random_state = 1126, kind = 'borderline-2')
                    self.data, self.target = smt.fit_resample(data[indices], target[indices])
                    from collections import Counter
                    print(sorted(Counter(self.target).items()))
                  
                # no upsampling
                else:
                    self.data = torch.FloatTensor(data[indices])
                    self.target = torch.LongTensor(target[indices])
            elif mode == 'val':
                indices = [i for i in range(len(data)) if i % 10 == 0]
                # Convert data into PyTorch tensors
                self.data = torch.FloatTensor(data[indices])
                self.target = torch.LongTensor(target[indices])

        self.dim = self.data.shape[1]

        print('Finished reading the {} set of Customer Dataset ({} samples found, each dim = {})'
              .format(mode, len(self.data), self.dim))

    def __getitem__(self, index):
        # Returns one sample at a time
        if self.mode in ['train', 'val']:
            # For training
            return self.data[index], self.target[index]
        else:
            # For testing (no target)
            return self.data[index]

    def __len__(self):
        # Returns the size of the dataset
        return len(self.data)

"""# **Deep Neural Network**"""
def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.weight.data.fill_(0.3)

class NeuralNet(nn.Module):
    ''' A simple fully-connected deep neural network '''
    def __init__(self, input_dim, hidden_size):
        super(NeuralNet, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 6)
        )
        self.net.apply(init_weights)

    def forward(self, x):
        ''' Given input of size (batch_size x input_dim), compute output of the network '''
        return self.net(x)
    


"""# **Hyperparameters, Loss function**

"""

# Hyperparameters
device = get_device()
os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/
num_epochs = 30000
batch_size = 32
learning_rate = 0.0002
momentum = 0.1
weight_decay = 0.01
amsgrad = False
# nn
hidden_size = 256
# loss function
criterion = nn.CrossEntropyLoss()

"""# **Train, Validation, Testing loop**

"""

#training loop
def train(model, train_loader, device, up):
  max_f1_score = 0
  model.train()
  for epoch in range(num_epochs):
      for i, (datas, labels) in enumerate(train_loader):  
          datas = datas.to(device)
          labels = labels.to(device)
          if up == False:
            labels = labels.squeeze(1) # needed when no upsampling

          # zero the gradients
          optimizer.zero_grad()
          # Forward pass
          outputs = model(datas)
          loss = criterion(outputs, labels)
          
          # Backward and optimize
          loss.backward()
          optimizer.step()
          # print training step
          # if (i+1) % 1000 == 0:
          #     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')
      # do validation after 5 epoch
      if (epoch+1) % 500 == 0:
        print('current epoch = {:4d} Loss = {:.4f}'
            .format(epoch+1, loss.item()))
        val_f1_score = val(model, val_loader, device)
        if val_f1_score > max_f1_score:
            # Save model if your model improved
            max_f1_score = val_f1_score
            print('Saving model (epoch = {:4d}, f1_score = {:.4f})'
                .format(epoch + 1, max_f1_score))
            torch.save(model.state_dict(), 'models/model.pth')

# validation loop
def val(model, val_loader, device):
  model.eval()
  with torch.no_grad():
      val_predictions = []
      for datas, labels in val_loader:
          datas = datas.to(device)
          labels = labels.to(device)
          labels = labels.squeeze(1)
          outputs = model(datas)
          # max returns (value ,index)
          _, predicted = torch.max(outputs.data, 1)
          predicted = predicted.cpu()
          predicted = np.array(predicted)
          for pred in predicted:
            val_predictions.append(pred)
      #evaluation      
      cm = confusion_matrix(dataset_val.target, val_predictions)
      print('CFmap')
      print(cm)
      print('f1 score')
      val_f1_score = f1_score(dataset_val.target, val_predictions, average = 'macro')
      print(val_f1_score)
  return val_f1_score

# testing loop
def test(model, test_loader, device):
  model.eval()
  val_f1_score = val(model, val_loader, device)
  print('best f1 score: {:.4f}'
        .format(val_f1_score))
  with torch.no_grad():
      test_predictions = []
      for datas in test_loader:
          datas = datas.to(device)

          outputs = model(datas)
          # max returns (value ,index)
          _, predicted = torch.max(outputs.data, 1)
          predicted = predicted.cpu()
          predicted = np.array(predicted)
          for pred in predicted:
            test_predictions.append(pred)
  make_pred(test_predictions, dataset_test.test_id)

"""# **Main**"""

# create dataset
dataset_train = CustomerDataset(tr_path, mode = 'train', up = True) # up => upsampling or not
dataset_val = CustomerDataset(tr_path, mode = 'val')
dataset_test = CustomerDataset(tt_path, mode = 'test', path2 = tr_path)
# create dataloader
train_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(dataset=dataset_val, batch_size=batch_size, shuffle=True, num_workers=2)
# don't shuffle the testing set!
test_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False, num_workers=2)
n_total_steps = len(train_loader)

# create model
model = NeuralNet(dataset_train.dim, hidden_size=hidden_size).to(device)
# Adam
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, amsgrad=amsgrad)
# SGD
# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
# Run!!!
train(model, train_loader, device, up = dataset_train.up)
# Load best model
del model
model = NeuralNet(dataset_train.dim, hidden_size=hidden_size).to(device)
ckpt = torch.load('models/model.pth', map_location='cpu')  
model.load_state_dict(ckpt)
test(model, test_loader, device)